{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the dataset samples before training the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I manually listened to the dataset recordings and I observed that the noise is usually voices in the background and rustling. Thus I will need a low pass filter to cut it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from   scipy.io import wavfile\n",
    "import scipy.signal as signalpy\n",
    "import wave\n",
    "import sys\n",
    "import math\n",
    "import contextlib\n",
    "import IPython\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import sklearn.datasets as datasets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from http://stackoverflow.com/questions/13728392/moving-average-or-running-mean\n",
    "def running_mean(x, windowSize):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[windowSize:] - cumsum[:-windowSize]) / windowSize\n",
    "\n",
    "# from http://stackoverflow.com/questions/2226853/interpreting-wav-data/2227174#2227174\n",
    "def interpret_wav(raw_bytes, n_frames, n_channels, sample_width, interleaved = True):\n",
    "\n",
    "    if sample_width == 1:\n",
    "        dtype = np.uint8 # unsigned char\n",
    "    elif sample_width == 2:\n",
    "        dtype = np.int16 # signed 2-byte short\n",
    "    else:\n",
    "        raise ValueError(\"Only supports 8 and 16 bit audio formats.\")\n",
    "\n",
    "    channels = np.frombuffer(raw_bytes, dtype=dtype)\n",
    "\n",
    "    if interleaved:\n",
    "        # channels are interleaved, i.e. sample N of channel M follows sample N of channel M-1 in raw data\n",
    "        channels.shape = (n_frames, n_channels)\n",
    "        channels = channels.T\n",
    "    else:\n",
    "        # channels are not interleaved. All samples from channel M occur before all samples from channel M-1\n",
    "        channels.shape = (n_channels, n_frames)\n",
    "\n",
    "    return channels\n",
    "\n",
    "\n",
    "def extract_audio(fname, tStart=None, tEnd=None):\n",
    "    with contextlib.closing(wave.open(fname,'rb')) as spf:\n",
    "        sampleRate = spf.getframerate()\n",
    "        ampWidth = spf.getsampwidth()\n",
    "        nChannels = spf.getnchannels()\n",
    "        nFrames = spf.getnframes()\n",
    "\n",
    "        startFrame, endFrame, segFrames = get_start_end_frames(nFrames, sampleRate, tStart, tEnd)\n",
    "\n",
    "        # Extract Raw Audio from multi-channel Wav File\n",
    "        spf.setpos(startFrame)\n",
    "        sig = spf.readframes(segFrames)\n",
    "        spf.close()\n",
    "\n",
    "        channels = interpret_wav(sig, segFrames, nChannels, ampWidth, True)\n",
    "\n",
    "        return (channels, nChannels, sampleRate, ampWidth, nFrames)\n",
    "    \n",
    "def convert_to_mono(channels, nChannels, outputType):\n",
    "    if nChannels == 2:\n",
    "        samples = np.mean(np.array([channels[0], channels[1]]), axis=0)  # Convert to mono\n",
    "    else:\n",
    "        samples = channels[0]\n",
    "\n",
    "    return samples.astype(outputType)\n",
    "\n",
    "def get_start_end_frames(nFrames, sampleRate, tStart=None, tEnd=None):\n",
    "\n",
    "    if tStart and tStart*sampleRate<nFrames:\n",
    "        start = tStart*sampleRate\n",
    "    else:\n",
    "        start = 0\n",
    "\n",
    "    if tEnd and tEnd*sampleRate<nFrames and tEnd*sampleRate>start:\n",
    "        end = tEnd*sampleRate\n",
    "    else:\n",
    "        end = nFrames\n",
    "\n",
    "    return (start,end,end-start)\n",
    "\n",
    "def plot_specgram(samples, sampleRate, tStart=None, tEnd=None):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.specgram(samples, Fs=sampleRate, NFFT=1024, noverlap=192, cmap='nipy_spectral', xextent=(tStart,tEnd))\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "    plt.show()\n",
    "\n",
    "def plot_audio_samples(title, samples, sampleRate, tStart=None, tEnd=None):\n",
    "    if not tStart:\n",
    "        tStart = 0\n",
    "\n",
    "    if not tEnd or tStart>tEnd:\n",
    "        tEnd = len(samples)/sampleRate\n",
    "\n",
    "    f, axarr = plt.subplots(2, sharex=True, figsize=(20,10))\n",
    "    axarr[0].set_title(title)\n",
    "    axarr[0].plot(np.linspace(tStart, tEnd, len(samples)), samples)\n",
    "    axarr[1].specgram(samples, Fs=sampleRate, NFFT=1024, noverlap=192, cmap='nipy_spectral', xextent=(tStart,tEnd))\n",
    "    #get_specgram(axarr[1], samples, sampleRate, tStart, tEnd)\n",
    "\n",
    "    axarr[0].set_ylabel('Amplitude')\n",
    "    axarr[1].set_ylabel('Frequency [Hz]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fir_low_pass(samples, fs, fL, N, outputType):\n",
    "    # Referece: https://fiiir.com\n",
    "\n",
    "    fL = fL / fs\n",
    "\n",
    "    # Compute sinc filter.\n",
    "    h = np.sinc(2 * fL * (np.arange(N) - (N - 1) / 2.))\n",
    "    # Apply window.\n",
    "    h *= np.hamming(N)\n",
    "    # Normalize to get unity gain.\n",
    "    h /= np.sum(h)\n",
    "    # Applying the filter to a signal s can be as simple as writing\n",
    "    s = np.convolve(samples, h).astype(outputType)\n",
    "    return s\n",
    "\n",
    "def fir_high_pass(samples, fs, fH, N, outputType):\n",
    "    # Referece: https://fiiir.com\n",
    "\n",
    "    fH = fH / fs\n",
    "\n",
    "    # Compute sinc filter.\n",
    "    h = np.sinc(2 * fH * (np.arange(N) - (N - 1) / 2.))\n",
    "    # Apply window.\n",
    "    h *= np.hamming(N)\n",
    "    # Normalize to get unity gain.\n",
    "    h /= np.sum(h)\n",
    "    # Create a high-pass filter from the low-pass filter through spectral inversion.\n",
    "    h = -h\n",
    "    h[int((N - 1) / 2)] += 1\n",
    "    # Applying the filter to a signal s can be as simple as writing\n",
    "    s = np.convolve(samples, h).astype(outputType)\n",
    "    return s\n",
    "\n",
    "def fir_band_reject(samples, fs, fL, fH, NL, NH, outputType):\n",
    "    # Referece: https://fiiir.com\n",
    "\n",
    "    fH = fH / fs\n",
    "    fL = fL / fs\n",
    "\n",
    "    # Compute a low-pass filter with cutoff frequency fL.\n",
    "    hlpf = np.sinc(2 * fL * (np.arange(NL) - (NL - 1) / 2.))\n",
    "    hlpf *= np.blackman(NL)\n",
    "    hlpf /= np.sum(hlpf)\n",
    "    # Compute a high-pass filter with cutoff frequency fH.\n",
    "    hhpf = np.sinc(2 * fH * (np.arange(NH) - (NH - 1) / 2.))\n",
    "    hhpf *= np.blackman(NH)\n",
    "    hhpf /= np.sum(hhpf)\n",
    "    hhpf = -hhpf\n",
    "    hhpf[int((NH - 1) / 2)] += 1\n",
    "    # Add both filters.\n",
    "    if NH >= NL:\n",
    "        h = hhpf\n",
    "        h[int((NH - NL) / 2) : int((NH - NL) / 2 + NL)] += hlpf\n",
    "    else:\n",
    "        h = hlpf\n",
    "        h[int((NL - NH) / 2) : int((NL - NH) / 2 + NH)] += hhpf\n",
    "    # Applying the filter to a signal s can be as simple as writing\n",
    "    s = np.convolve(samples, h).astype(outputType)\n",
    "\n",
    "    return s\n",
    "\n",
    "def fir_band_pass(samples, fs, fL, fH, NL, NH, outputType):\n",
    "    # Referece: https://fiiir.com\n",
    "\n",
    "    fH = fH / fs\n",
    "    fL = fL / fs\n",
    "\n",
    "    # Compute a low-pass filter with cutoff frequency fH.\n",
    "    hlpf = np.sinc(2 * fH * (np.arange(NH) - (NH - 1) / 2.))\n",
    "    hlpf *= np.blackman(NH)\n",
    "    hlpf /= np.sum(hlpf)\n",
    "    # Compute a high-pass filter with cutoff frequency fL.\n",
    "    hhpf = np.sinc(2 * fL * (np.arange(NL) - (NL - 1) / 2.))\n",
    "    hhpf *= np.blackman(NL)\n",
    "    hhpf /= np.sum(hhpf)\n",
    "    hhpf = -hhpf\n",
    "    hhpf[int((NL - 1) / 2)] += 1\n",
    "    # Convolve both filters.\n",
    "    h = np.convolve(hlpf, hhpf)\n",
    "    # Applying the filter to a signal s can be as simple as writing\n",
    "    s = np.convolve(samples, h).astype(outputType)\n",
    "\n",
    "    return s\n",
    "\n",
    "def butter_low_pass(samples, fs, fL, order, outputType):\n",
    "    # Normalize the cutoff frequency\n",
    "    fN = fs / 2\n",
    "    fL_norm = fL / fN\n",
    "\n",
    "    # Calculate the Butterworth filter coefficients\n",
    "    b, a = signalpy.butter(order, fL_norm, btype='low')\n",
    "\n",
    "    # Apply the filter to the signal\n",
    "    s = signalpy.lfilter(b, a, samples).astype(outputType)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'phonocardiogram_dataset/training_data/training_data/2530_AV.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m tStart \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      4\u001b[0m tEnd \u001b[39m=\u001b[39m \u001b[39m30\u001b[39m\n\u001b[1;32m----> 6\u001b[0m channels, nChannels, sampleRate, ampWidth, nFrames \u001b[39m=\u001b[39m extract_audio(\u001b[39m'\u001b[39;49m\u001b[39mphonocardiogram_dataset/training_data/training_data/2530_AV.wav\u001b[39;49m\u001b[39m'\u001b[39;49m, tStart, tEnd)\n\u001b[0;32m      7\u001b[0m samples \u001b[39m=\u001b[39m convert_to_mono(channels, nChannels, np\u001b[39m.\u001b[39mint16)\n\u001b[0;32m      9\u001b[0m plot_audio_samples(\u001b[39m\"\u001b[39m\u001b[39mFirst heart sample\u001b[39m\u001b[39m\"\u001b[39m, samples, sampleRate, tStart, tEnd)\n",
      "Cell \u001b[1;32mIn[3], line 30\u001b[0m, in \u001b[0;36mextract_audio\u001b[1;34m(fname, tStart, tEnd)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_audio\u001b[39m(fname, tStart\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, tEnd\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 30\u001b[0m     \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mclosing(wave\u001b[39m.\u001b[39;49mopen(fname,\u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m)) \u001b[39mas\u001b[39;00m spf:\n\u001b[0;32m     31\u001b[0m         sampleRate \u001b[39m=\u001b[39m spf\u001b[39m.\u001b[39mgetframerate()\n\u001b[0;32m     32\u001b[0m         ampWidth \u001b[39m=\u001b[39m spf\u001b[39m.\u001b[39mgetsampwidth()\n",
      "File \u001b[1;32mc:\\Users\\Ema\\anaconda3\\envs\\am\\lib\\wave.py:509\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(f, mode)\u001b[0m\n\u001b[0;32m    507\u001b[0m         mode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    508\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 509\u001b[0m     \u001b[39mreturn\u001b[39;00m Wave_read(f)\n\u001b[0;32m    510\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    511\u001b[0m     \u001b[39mreturn\u001b[39;00m Wave_write(f)\n",
      "File \u001b[1;32mc:\\Users\\Ema\\anaconda3\\envs\\am\\lib\\wave.py:159\u001b[0m, in \u001b[0;36mWave_read.__init__\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_i_opened_the_file \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 159\u001b[0m     f \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_i_opened_the_file \u001b[39m=\u001b[39m f\n\u001b[0;32m    161\u001b[0m \u001b[39m# else, assume it is an open file object already\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'phonocardiogram_dataset/training_data/training_data/2530_AV.wav'"
     ]
    }
   ],
   "source": [
    "# 2530_AV.wav\n",
    "\n",
    "tStart = 0\n",
    "tEnd = 30\n",
    "\n",
    "channels, nChannels, sampleRate, ampWidth, nFrames = extract_audio('phonocardiogram_dataset/training_data/training_data/2530_AV.wav', tStart, tEnd)\n",
    "samples = convert_to_mono(channels, nChannels, np.int16)\n",
    "\n",
    "plot_audio_samples(\"First heart sample\", samples, sampleRate, tStart, tEnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = 'phonocardiogram_dataset/training_data/training_data/2530_AV.wav'\n",
    "\n",
    "IPython.display.Audio( audio_file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = 'phonocardiogram_dataset/training_data/training_data/2530_AV.wav'\n",
    "\n",
    "tStart=0\n",
    "tEnd=23\n",
    "\n",
    "channels, nChannels, sampleRate, ampWidth, nFrames = extract_audio(audio_file, tStart, tEnd)\n",
    "samples = convert_to_mono(channels, nChannels, np.int16)\n",
    "\n",
    "#Trece jos\n",
    "\n",
    "# sample_low_pass = fir_low_pass(samples, sampleRate , 200 , 551, np.int16)\n",
    "sample_low_pass = butter_low_pass(samples, sampleRate , 200 , 19, np.int16)\n",
    "plot_audio_samples(\"Trece-jos_Butterworth\", sample_low_pass, sampleRate, tStart, tEnd)\n",
    "\n",
    "# sample_low_pass_amplified =  np.multiply(sample_low_pass, 4)\n",
    "\n",
    "# plot_audio_samples(\"Amplificat\", sample_low_pass_amplified, sampleRate, tStart, tEnd)\n",
    "\n",
    "wavfile.write('phonocardiogram_dataset/training_data/filtered_data/2530_AV_filtered.wav', sampleRate, sample_low_pass)\n",
    "\n",
    "\n",
    "\n",
    "IPython.display.Audio( 'phonocardiogram_dataset/training_data/filtered_data/2530_AV_filtered.wav' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = 'phonocardiogram_dataset/training_data/training_data/9979_AV.wav'\n",
    "\n",
    "tStart=0\n",
    "tEnd=18\n",
    "\n",
    "channels, nChannels, sampleRate, ampWidth, nFrames = extract_audio(audio_file, tStart, tEnd)\n",
    "samples = convert_to_mono(channels, nChannels, np.int16)\n",
    "\n",
    "plot_audio_samples(\"Initial\", samples, sampleRate, tStart, tEnd)\n",
    "\n",
    "#Trece jos\n",
    "\n",
    "# sample_low_pass = fir_low_pass(samples, sampleRate , 200 , 551, np.int16)\n",
    "sample_low_pass = butter_low_pass(samples, sampleRate , 200 , 19, np.int16)\n",
    "plot_audio_samples(\"Trece-jos_Butterworth\", sample_low_pass, sampleRate, tStart, tEnd)\n",
    "\n",
    "# sample_low_pass_amplified =  np.multiply(sample_low_pass, 4)\n",
    "\n",
    "# plot_audio_samples(\"Amplificat\", sample_low_pass_amplified, sampleRate, tStart, tEnd)\n",
    "\n",
    "wavfile.write('phonocardiogram_dataset/training_data/filtered_data/9979_AV_filtered.wav', sampleRate, sample_low_pass)\n",
    "\n",
    "\n",
    "\n",
    "IPython.display.Audio( 'phonocardiogram_dataset/training_data/filtered_data/9979_AV_filtered.wav' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory paths\n",
    "input_dir = 'phonocardiogram_dataset/training_data/training_data'\n",
    "output_dir = 'phonocardiogram_dataset/training_data/filtered_data'\n",
    "\n",
    "\n",
    "# Set the filter parameters\n",
    "fL = 200\n",
    "order = 19\n",
    "outputType = np.int16\n",
    "\n",
    "# Iterate over all files in the input directory\n",
    "\n",
    "for file_name in os.listdir(input_dir):\n",
    "    # Check if the file is a .wav file\n",
    "    if file_name.endswith('.wav'):\n",
    "        # Set the input and output file paths\n",
    "        input_path = os.path.join(input_dir, file_name)\n",
    "        output_path = os.path.join(output_dir, file_name[:-4] + '_filtered.wav') # I added the _filtered suffix to the output file name\n",
    "        \n",
    "        # Load the audio file and apply the filter\n",
    "        sample_rate, samples = wavfile.read(input_path)\n",
    "        sample_low_pass = butter_low_pass(samples, sample_rate, fL, order, outputType)\n",
    "        \n",
    "        # Save the filtered audio to a new file\n",
    "        wavfile.write(output_path, sample_rate, sample_low_pass)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'phonocardiogram_dataset/training_data/training_data'\n",
    "output_dir = 'phonocardiogram_dataset/training_data/filtered_data'\n",
    "file_name = '13918_AV'\n",
    "\n",
    "IPython.display.Audio(os.path.join(input_dir , file_name + '.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(os.path.join(output_dir , file_name + '_filtered.wav'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "am",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
